{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f008fdbe-2fe0-48fb-bf66-f6929cec764b",
   "metadata": {},
   "source": [
    "# Final Assignment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a481c7-59ba-475b-9b2c-c9e2bf942f82",
   "metadata": {},
   "source": [
    "## Instuctions\n",
    "* An extra 20 points for those who implement ALL the functions themselves without using tensorflows tf.keras.preprocessing and TextVectorization.\n",
    "* Use pyarrow to save the embedding map.\n",
    "* Find most similar function should be efficient. 20 sec to wait for a result is too much. You should aim to < 2 sec (use \"timeit\" magic to verify)\n",
    "* Output cells should not be too big. DO NOT DUMP A LOT OF DATA IN THE OUTPUT. NOTEOOKS THAT WON'T FOLLOW THIS INSTRUCTION WILL NOT BE CHECKED AND THEIR GRADE WILL BE SET TO ZERO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031f54b0-560c-4d23-95a7-614f71b40334",
   "metadata": {},
   "source": [
    "### Read dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b7098bf-e86b-439d-bf82-ef06c6d55895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in zip: ['review_230k.parquet']\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "zip_path = 'data/review_230k.zip'\n",
    "\n",
    "def extract_parquet_from_zip(zip_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        file_list = zip_ref.namelist()\n",
    "        print(f\"Files in zip: {file_list}\") \n",
    "\n",
    "        parquet_file_name = [f for f in file_list if f.endswith('.parquet')][0]\n",
    "\n",
    "        with zip_ref.open(parquet_file_name) as file:\n",
    "            df = pd.read_parquet(file)\n",
    "    return df\n",
    "\n",
    "data = extract_parquet_from_zip(zip_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d2abf69-dec8-450b-89b5-594a7226cae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data: pd.DataFrame):\n",
    "    \"\"\" Create the dataset in your preferrable format \"\"\"\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58c43b7a-abf6-4ef2-8f04-e3b7d5d32b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                title  \\\n",
      "0             Truly is \"Jewel of the Upper Wets Side\"   \n",
      "1                             My home away from home!   \n",
      "2                                          Great Stay   \n",
      "3                                  Modern Convenience   \n",
      "4       Its the best of the Andaz Brand in the US....   \n",
      "...                                               ...   \n",
      "230334                        Treated us like royalty   \n",
      "230335        Fine time but some room for improvement   \n",
      "230336                Great venue for corporate event   \n",
      "230337                             Almost in the Loop   \n",
      "230338                                    Comfortable   \n",
      "\n",
      "                                                     text  \n",
      "0       Stayed in a king suite for 11 nights and yes i...  \n",
      "1       On every visit to NYC, the Hotel Beacon is the...  \n",
      "2       This is a great property in Midtown. We two di...  \n",
      "3       The Andaz is a nice hotel in a central locatio...  \n",
      "4       I have stayed at each of the US Andaz properti...  \n",
      "...                                                   ...  \n",
      "230334  My kids (17 and 13) stayed at this location De...  \n",
      "230335  Just returned from a fabulous wknd in NYC. The...  \n",
      "230336  I organized a corporate event at this hotel, a...  \n",
      "230337  We had a beautiful room with a balcony and gre...  \n",
      "230338  This hotel is located in Greek Town. Stayed he...  \n",
      "\n",
      "[230339 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset = create_dataset(data)\n",
    "\n",
    "#\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "964d4065-2935-4b64-bff5-a47134baa9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great Stay\n",
      "Stayed in a king suite for 11 nights and yes it cots us a bit but we were happy with the standard of room, the location and the friendliness of the staff. Our room was on the 20th floor overlooking Broadway and the madhouse of the Fairway Market. Room was quite with no noise evident from the hallway or adjoining rooms. It was great to be able to open windows when we craved fresh rather than heated air. The beds, including the fold out sofa bed, were comfortable and the rooms were cleaned well. Wi-fi access worked like a dream with only one connectivity issue on our first night and this was promptly responded to with a call from the service provider to ensure that all was well. The location close to the 72nd Street subway station is great and the complimentary umbrellas on the drizzly days were greatly appreciated. It is fabulous to have the kitchen with cooking facilities and the access to a whole range of fresh foods directly across the road at Fairway.\n",
      "This is the second time that members of the party have stayed at the Beacon and it will certainly be our hotel of choice for future visits.\n"
     ]
    }
   ],
   "source": [
    "first_title_value = dataset['text'].iloc[1]\n",
    "print(dataset['title'].iloc[2])\n",
    "print(dataset['text'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c4e719-a0c9-4ce3-8feb-e0ec551b4c76",
   "metadata": {},
   "source": [
    "### Clean and standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbdb4643-5bde-4356-8576-3077c66ea0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stayed', 'in', 'a', 'king', 'suite', 'for', 'nights', 'and', 'yes', 'it', 'cots', 'us', 'a', 'bit', 'but', 'we', 'were', 'happy', 'with', 'the', 'standard', 'of', 'room', 'the', 'location', 'and', 'the', 'friendliness', 'of', 'the', 'staff']\n",
      "['our', 'room', 'was', 'on', 'the', 'th', 'floor', 'overlooking', 'broadway', 'and', 'the', 'madhouse', 'of', 'the', 'fairway', 'market']\n",
      "['room', 'was', 'quite', 'with', 'no', 'noise', 'evident', 'from', 'the', 'hallway', 'or', 'adjoining', 'rooms']\n",
      "['it', 'was', 'great', 'to', 'be', 'able', 'to', 'open', 'windows', 'when', 'we', 'craved', 'fresh', 'rather', 'than', 'heated', 'air']\n",
      "['the', 'beds', 'including', 'the', 'fold', 'out', 'sofa', 'bed', 'were', 'comfortable', 'and', 'the', 'rooms', 'were', 'cleaned', 'well']\n",
      "['wifi', 'access', 'worked', 'like', 'a', 'dream', 'with', 'only', 'one', 'connectivity', 'issue', 'on', 'our', 'first', 'night', 'and', 'this', 'was', 'promptly', 'responded', 'to', 'with', 'a', 'call', 'from', 'the', 'service', 'provider', 'to', 'ensure', 'that', 'all', 'was', 'well']\n",
      "['the', 'location', 'close', 'to', 'the', 'nd', 'street', 'subway', 'station', 'is', 'great', 'and', 'the', 'complimentary', 'umbrellas', 'on', 'the', 'drizzly', 'days', 'were', 'greatly', 'appreciated']\n",
      "['it', 'is', 'fabulous', 'to', 'have', 'the', 'kitchen', 'with', 'cooking', 'facilities', 'and', 'the', 'access', 'to', 'a', 'whole', 'range', 'of', 'fresh', 'foods', 'directly', 'across', 'the', 'road', 'at', 'fairway']\n",
      "['this', 'is', 'the', 'second', 'time', 'that', 'members', 'of', 'the', 'party', 'have', 'stayed', 'at', 'the', 'beacon', 'and', 'it', 'will', 'certainly', 'be', 'our', 'hotel', 'of', 'choice', 'for', 'future', 'visits']\n",
      "['on', 'every', 'visit', 'to', 'nyc', 'the', 'hotel', 'beacon', 'is', 'the', 'place', 'we', 'love', 'to', 'stay']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import string\n",
    "\n",
    "def prepare_dataset(dataset):\n",
    "    \"\"\" Clean and prepare the dataset before word encoding, returning a single 2D array \"\"\"\n",
    "    \n",
    "    dataset['text'] = dataset['text'].fillna('')\n",
    "    \n",
    "    def clean_text(text):\n",
    "        text = text.lower()\n",
    "        sentences = text.split('.')\n",
    "        cleaned_sentences = [\n",
    "            sentence.translate(str.maketrans('', '', string.punctuation + string.digits)).strip()\n",
    "            for sentence in sentences if sentence\n",
    "        ]\n",
    "        \n",
    "        sentence_word_arrays = [sentence.split() for sentence in cleaned_sentences]\n",
    "        return sentence_word_arrays\n",
    "    dataset['tokenized_sentences'] = dataset['text'].apply(clean_text)\n",
    "    all_tokenized_sentences = []\n",
    "    for sentence_list in dataset['tokenized_sentences']:\n",
    "        all_tokenized_sentences.extend(sentence_list)\n",
    "    \n",
    "    return all_tokenized_sentences\n",
    "\n",
    "all_tokenized_sentences = prepare_dataset(dataset)\n",
    "\n",
    "for sentence in all_tokenized_sentences[:10]:\n",
    "    print(sentence) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d4fd16-3787-411e-b098-32a1de657f71",
   "metadata": {},
   "source": [
    "### Create A vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a56e5ae3-712a-4a30-8b50-a7f5ac3cf684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UNK]: 0\n",
      "stayed: 1\n",
      "in: 2\n",
      "a: 3\n",
      "king: 4\n",
      "suite: 5\n",
      "for: 6\n",
      "nights: 7\n",
      "and: 8\n",
      "yes: 9\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    " \n",
    "def create_vocabulary(all_tokenized_sentences) -> dict[str, int]:\n",
    " \n",
    "    vocabulary = {'[UNK]': 0}\n",
    "    index = 1\n",
    "    for sentence in all_tokenized_sentences:\n",
    "        for word in sentence:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = index\n",
    "                index += 1\n",
    "    return vocabulary\n",
    "\n",
    "all_tokenized_sentences = prepare_dataset(dataset)\n",
    "\n",
    "\n",
    "vocabulary = create_vocabulary(all_tokenized_sentences)\n",
    "\n",
    "n = 10\n",
    "for i, (key, value) in enumerate(vocabulary.items()):\n",
    "    if i >= n:\n",
    "        break\n",
    "    print(f\"{key}: {value}\")\n",
    "print(type(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfa0b49-60b8-407e-9d6b-c5eef5f2e28e",
   "metadata": {},
   "source": [
    "### Word encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40e66a17-f60e-4686-a360-2e4a483ed2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences(all_tokenized_sentences, vocabulary: dict[str, int]):\n",
    "    \"\"\"\n",
    "    Encodes each word in all_tokenized_sentences based on the provided vocabulary.\n",
    "    \n",
    "    Parameters:\n",
    "    - all_tokenized_sentences: List of tokenized sentences (each sentence is a list of words).\n",
    "    - vocabulary: Dictionary mapping words to their corresponding integer indices.\n",
    "    \n",
    "    Returns:\n",
    "    - encoded_sentences: List of encoded sentences (each sentence is a list of integer indices).\n",
    "    \"\"\"\n",
    "    encoded_sentences = []\n",
    "    for sentence in all_tokenized_sentences:\n",
    "        encoded_sentence = [vocabulary.get(word, vocabulary.get('<UNK>')) for word in sentence]\n",
    "        encoded_sentences.append(encoded_sentence)\n",
    "    return encoded_sentences\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18a41657-004a-4a20-9e11-f055656760ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 3, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 19, 23, 8, 19, 24, 21, 19, 25]\n",
      "[26, 22, 27, 28, 19, 29, 30, 31, 32, 8, 19, 33, 21, 19, 34, 35]\n",
      "[22, 27, 36, 18, 37, 38, 39, 40, 19, 41, 42, 43, 44]\n",
      "[10, 27, 45, 46, 47, 48, 46, 49, 50, 51, 15, 52, 53, 54, 55, 56, 57]\n",
      "[19, 58, 59, 19, 60, 61, 62, 63, 16, 64, 8, 19, 44, 16, 65, 66]\n"
     ]
    }
   ],
   "source": [
    "encoded_sentences = encode_sentences(all_tokenized_sentences, vocabulary)\n",
    "for encoded_sentence in encoded_sentences[:5]:\n",
    "    print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb906ef-e136-4b0a-a083-68370c27b530",
   "metadata": {},
   "source": [
    "Example using tensoflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb7c8aad-4117-4d28-ad70-faf43b31b695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create a TensorFlow dataset from the 'text' column\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(dataset['text'])\n",
    "\n",
    "# Create the vocabulary and convert it to a list\n",
    "vocabulary = create_vocabulary(all_tokenized_sentences)\n",
    "vocabulary_list = list(vocabulary.keys())\n",
    "\n",
    "# Set up the TextVectorization layer\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    max_tokens=258000,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=100,\n",
    "    vocabulary=vocabulary_list\n",
    ")\n",
    "\n",
    "# Get the updated vocabulary\n",
    "vocabulary = vectorize_layer.get_vocabulary()\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Encode the text data\n",
    "encoded_text_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n",
    "encoded_dataset = encoded_text_ds.as_numpy_iterator()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c03d49b6-c033-4dad-ba1f-72fb2438d86c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 257009\n",
      "Original text: Stayed in a king suite for 11 nights and yes it cots us a bit but we were happy with the standard of room, the location and the friendliness of the staff. Our room was on the 20th floor overlooking Broadway and the madhouse of the Fairway Market. Room was quite with no noise evident from the hallway or adjoining rooms. It was great to be able to open windows when we craved fresh rather than heated air. The beds, including the fold out sofa bed, were comfortable and the rooms were cleaned well. Wi-fi access worked like a dream with only one connectivity issue on our first night and this was promptly responded to with a call from the service provider to ensure that all was well. The location close to the 72nd Street subway station is great and the complimentary umbrellas on the drizzly days were greatly appreciated. It is fabulous to have the kitchen with cooking facilities and the access to a whole range of fresh foods directly across the road at Fairway.\n",
      "This is the second time that members of the party have stayed at the Beacon and it will certainly be our hotel of choice for future visits.\n",
      "Encoded text: [ 3  4  5  6  7  8  1  9 10 11 12 13 14  5 15 16 17 18 19 20 21 22 23 24\n",
      " 21 25 10 21 26 23 21 27 28 24 29 30 21  1 32 33 34 10 21 35 23 21 36 37\n",
      " 24 29 38 20 39 40 41 42 21 43 44 45 46 12 29 47 48 49 50 48 51 52 53 17\n",
      " 54 55 56 57 58 59 21 60 61 21 62 63 64 65 18 66 10 21 46 18 67 68 69 70\n",
      " 71 72  5 73]\n",
      "--------------------------------------------------\n",
      "Original text: On every visit to NYC, the Hotel Beacon is the place we love to stay. So conveniently located to Central Park, Lincoln Center and great local restaurants. The rooms are lovely - beds so comfortable, a great little kitchen and new wizz bang coffee maker. The staff are so accommodating and just love walking across the street to the Fairway supermarket with every imaginable goodies to eat (if you choose not to go out for every meal!)\n",
      "Encoded text: [ 30 124 125  48 126  21 120 117  94  21 127  17 128  48 129 130 131 132\n",
      "  48 133 134 135 136  10  47 137 138  21  46 139 140  60 130  66   5  47\n",
      " 141 103  10 142 143 144 145 146  21  27 139 130 147  10 148 128 149 110\n",
      "  21  91  48  21  36 150  20 124 151 152  48 153 154 155 156 157  48 158\n",
      "  63   8 124 159   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n",
      "--------------------------------------------------\n",
      "Original text: This is a great property in Midtown. We two different rooms different rooms during our stay. The first room was in the North tower, which was quite inconvenient. You have to go through the conference area to get to the north elevators. \n",
      "The second room was the Andaz Suite. It was nicely appointed room, but the best part about it was the bathroom. From the foot soaking bowl to the bath products, everything about the bathroom was awesome!\n",
      "Lemon poppy-seed pancakes are must haves at the restaurant. One of the best pancakes ever.\n",
      "Encoded text: [ 80  94   5  47 160   4 161  17 162 163  46 163  46 164  28 129  21  78\n",
      "  24  29   4  21 165 166 167  29  38 168 155 102  48 158 169  21 170 171\n",
      "  48 172  48  21 165 173  21 113  24  29  21 174   7  12  29 175 176  24\n",
      "  16  21 177 178 179  12  29  21 180  42  21 181 182 183  48  21 184 185\n",
      " 186 179  21 180  29 187 188 189 190 139 191 192 112  21 193  75  23  21\n",
      " 177 190 194   0   0   0   0   0   0   0]\n",
      "--------------------------------------------------\n",
      "Original text: The Andaz is a nice hotel in a central location of Manhattan. The Hyatt has come up with a modern hotel that is both comfortable and convenient. When you arrive you are greeted by friendly \"Hosts\" and they walk you to the check-in desk while offering you a beverage. \n",
      "We had a one bedroom suite that accommodated four people reasonably well. Plenty of closet space, well lit with floor to ceiling windows, and actually quiet!\n",
      "The bathroom was large with a very nice walk-in shower and a built-in bench with unique low spout to wash your feet.\n",
      "The kitchenette was a nice touch with a stocked fridge offering complimentary non-alcoholic beverages and snacks, dishes and utensils, a sink, dishwasher, and a microwave. \n",
      "They have daily Happy Hour(s) where you can get a complimentary decent glass of wine in the modest Lobby Lounge and bring it to your room. The Lobby Lounge has some seating and one table with 8 or so chairs where you can buy food from the adjacent restaurant. One suggestion is to offer selections of cheese and crackers platters to go with that wine. We ordered one that had to be custom made, but it worked well. \n",
      "We didn't eat in the hotel restaurants. The restaurant off the lobby was very pricey for breakfast (+$20 per entree). When you can get a decent breakfast within a block or two of the hotel for under $10, what can I say?\n",
      "As a hotel designer and someone who is very picky about hotels, I will definitely stay there again and I highly recommend it.\n",
      "Encoded text: [ 21 174  94   5 195 120   4   5 133  25  23 196  21 197 198 199 200  20\n",
      "   5 201 120  87  94 202  66  10 203  53 155 204 155 139 205 206 207 208\n",
      "  10 209 210 155  48  21 211 212 213 214 155   5 215  17 216   5  75 217\n",
      "   7  87 218 219 220 221  68 222  23 223 224  68 225  20  32  48 226  52\n",
      "  10 227 228  21 180  29 229  20   5 230 195 231 232  10   5 233 234  20\n",
      " 235 236 237  48 238 239 240  21 241  29]\n",
      "--------------------------------------------------\n",
      "Original text: I have stayed at each of the US Andaz properties, and this one is the best of them all. MUCH better than the West Hollywood property where Brand standards are slowly fading. This is the shining star here in the States.\n",
      "My room (suite) on the 12th floor was fantastic as usual. Perfectly maintained, very comfortable, great bathrooms (best in NYC???). The bathroom is huge, with large shower 'room' and multiple shower heads. Double lavatories flank one wall. Huge....\n",
      "THe bed is always perfect. Televisions in both rooms were perfect for my limited time spend in the suite. Nice desk for work area and in room dining. \n",
      "Every room has a fridge with complimentary soft drinks, snacks, water, sparkling water, milk and juices. Nice touch. Suite has full size refrigerator with icemaker, and also features a microwave, sink and dishwasher drawer....all nicely concealed behind beautiful cabinetry. Really great for a longer stay...\n",
      "I do have 2 complaints. First, the elevators. The waits were sometimes ridiculous. 3 elevators serve the main tower with a rather limited number of rooms. THis is a newer hotel, but there is no service elevator. I was continually surprised after waiting for an elevator at how many stops we made for housekeeping to enter with carts, room service carts and trays, etc. How could Hyatt skip a service elevator on this new property? It really stinks at times.....its bad.\n",
      "Lastly, i have traveled with world and certainly know NYC pricing. Breakfast at the Andaz ranks above almost anything i have ever paid. Apparently, i had a $190.00 breakfast for two the first morning, and $175.00 the second. Eggs for each, coffee, berries. Wow. Of course, i really don't know...as there is no bill. They just deliver it, and leave. Prices are listed in the in-suite menu, but no itemized billing is given. I am 'gifted' with a $75.00 credit for my breakfast as a Diamond member, and i assumed that had not been applied when my bill reflected a 'non itemized' charge for $110 plus for the first night, and slightly less for the second. I was wrong, it had been applied (no way for me to see this). WOW. It certainly isn't worth it.\n",
      "I know i sound like one of those guys who complains about pricing, but clearly doesn't know what things cost. Wrong. This one really surprised me, and i have traveled the world for more than 20 years staying in some of the finest hotels in the world. Andaz is great, but it doesn't rank with many that i have had the pleasure of staying with.\n",
      "I just had breakfast this morning for 2 at the Four Seasons NYC. It was half the cost....almost. :)\n",
      "Stay away from room service in my opinion....\n",
      "Encoded text: [290 102   3 112 303  23  21  14 174 304  10  80  75  94  21 177  23 305\n",
      "  88 306 307  57  21 308 309 160 255 310 311 139 312 313  80  94  21 314\n",
      " 315 316   4  21 317 318  24   7  30  21   1  32  29 319 292 320 321 322\n",
      " 230  66  47 323 177   4 126  21 180  94 324  20 229 232  24  10 325 232\n",
      " 326 327 328 329  75 330 324  21  65  94 331 332 333   4 202  46  18 332\n",
      "   8 318 334 114 335   4  21   7 195 212]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "# Convert the encoded dataset to a list\n",
    "encoded_dataset = list(encoded_text_ds.as_numpy_iterator())\n",
    "# Display a few encoded texts\n",
    "for original_text, encoded_text in zip(dataset['text'], encoded_dataset[:5]):\n",
    "    print(f\"Original text: {original_text}\")\n",
    "    print(f\"Encoded text: {encoded_text}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1cb0a0-78a4-469a-8081-f325d97c8974",
   "metadata": {},
   "source": [
    "### Generate Positive and Negative Pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa8aa644-8408-4d9d-86cf-3e134db00ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs generated: 272260698\n",
      "Example pair and label: [3 4], 1\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def generate_training_data_optimized(encoded_dataset, number_of_neg_samples: int, window_size: int = 2):\n",
    "    \"\"\"\n",
    "    Generate positive and negative pairs for Word2Vec training in an optimized way.\n",
    "\n",
    "    Parameters:\n",
    "    - encoded_dataset: List of lists, where each sublist contains word indices representing a sentence.\n",
    "    - number_of_neg_samples: Number of negative samples per positive pair.\n",
    "    - window_size: The context window size around the target word.\n",
    "\n",
    "    Returns:\n",
    "    - pairs: Numpy array with shape (num_samples, 2) containing target and context word indices.\n",
    "    - labels: Numpy array with shape (num_samples,) containing labels (1 for positive pairs, 0 for negative pairs).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate word frequencies\n",
    "    word_freq = defaultdict(int)\n",
    "    for sentence in encoded_dataset:\n",
    "        for word in sentence:\n",
    "            word_freq[word] += 1\n",
    "\n",
    "    total_words = sum(word_freq.values())\n",
    "\n",
    "    # Calculate the probability distribution for negative sampling\n",
    "    word_prob = np.array([(freq / total_words) ** 0.75 for freq in word_freq.values()])\n",
    "    words = np.array(list(word_freq.keys()))\n",
    "    word_prob /= word_prob.sum()\n",
    "\n",
    "    # Generate positive pairs\n",
    "    target_words, context_words = [], []\n",
    "    for sentence in encoded_dataset:\n",
    "        sentence_length = len(sentence)\n",
    "        for i, target_word in enumerate(sentence):\n",
    "            start = max(i - window_size, 0)\n",
    "            end = min(i + window_size + 1, sentence_length)\n",
    "            context = list(sentence[start:i]) + list(sentence[i + 1:end])\n",
    "            target_words.extend([target_word] * len(context))\n",
    "            context_words.extend(context)\n",
    "\n",
    "    # Convert lists to NumPy arrays\n",
    "    target_words = np.array(target_words)\n",
    "    context_words = np.array(context_words)\n",
    "\n",
    "    # Generate negative samples\n",
    "    num_positive = len(target_words)\n",
    "    negative_samples = np.random.choice(words, size=num_positive * number_of_neg_samples, p=word_prob)\n",
    "    negative_samples = negative_samples.reshape(num_positive, number_of_neg_samples)\n",
    "\n",
    "    # Create negative pairs\n",
    "    target_neg = np.repeat(target_words, number_of_neg_samples)\n",
    "    negative_words = negative_samples.flatten()\n",
    "\n",
    "    # Combine positive and negative pairs\n",
    "    pairs_positive = np.stack([target_words, context_words], axis=1)\n",
    "    pairs_negative = np.stack([target_neg, negative_words], axis=1)\n",
    "\n",
    "    pairs = np.concatenate([pairs_positive, pairs_negative], axis=0)\n",
    "    labels = np.concatenate([np.ones(len(pairs_positive), dtype=np.int32), np.zeros(len(pairs_negative), dtype=np.int32)])\n",
    "\n",
    "    return pairs, labels\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "number_of_neg_samples = 2\n",
    "window_size = 2\n",
    "\n",
    "pairs, labels = generate_training_data_optimized(encoded_dataset, number_of_neg_samples, window_size)\n",
    "\n",
    "print(f\"Total pairs generated: {len(pairs)}\")\n",
    "print(f\"Example pair and label: {pairs[0]}, {labels[0]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ed1b7bc-a498-471e-a94f-a5bb2ba05fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data saved to training_data.parquet\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Function to save pairs and labels using PyArrow\n",
    "def save_training_data_parquet(pairs: np.ndarray, labels: np.ndarray, file_path: str):\n",
    "    \"\"\"\n",
    "    Saves pairs and labels to a file using PyArrow in Parquet format.\n",
    "    \n",
    "    Parameters:\n",
    "    - pairs: Numpy array containing pairs (target, context/negative).\n",
    "    - labels: Numpy array containing labels (1 for positive pairs, 0 for negative pairs).\n",
    "    - file_path: The file path where data should be saved.\n",
    "    \"\"\"\n",
    "    # Create a PyArrow table\n",
    "    table = pa.Table.from_pydict({\n",
    "        'target_word': pairs[:, 0],\n",
    "        'context_word': pairs[:, 1],\n",
    "        'label': labels\n",
    "    })\n",
    "    \n",
    "    # Save the table in Parquet format\n",
    "    pq.write_table(table, file_path)\n",
    "    print(f\"Training data saved to {file_path}\")\n",
    "\n",
    "training_data_file = 'training_data.parquet'\n",
    "save_training_data_parquet(pairs, labels, training_data_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bab6a659-a4db-4452-bd11-97e5c3d38aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded table shape: (272260698, 3)\n",
      "   target_word  context_word  label\n",
      "0            3             4      1\n",
      "1            3             5      1\n",
      "2            4             3      1\n",
      "3            4             5      1\n",
      "4            4             6      1\n"
     ]
    }
   ],
   "source": [
    "# Loading the saved data\n",
    "loaded_table = pq.read_table('training_data.parquet')\n",
    "loaded_df = loaded_table.to_pandas()\n",
    "\n",
    "print(f\"Loaded table shape: {loaded_df.shape}\")\n",
    "print(loaded_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2d6efd-b902-40ab-a000-6e4f840217a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba2033bb-2f06-45e4-943d-c0afa4929222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample pairs with decoding:\n",
      "1. Positive pair: (stayed, in)\n",
      "2. Positive pair: (stayed, a)\n",
      "3. Positive pair: (in, stayed)\n",
      "4. Positive pair: (in, a)\n",
      "5. Positive pair: (in, king)\n",
      "6. Positive pair: (a, stayed)\n",
      "7. Positive pair: (a, in)\n",
      "8. Positive pair: (a, king)\n",
      "9. Positive pair: (a, suite)\n",
      "10. Positive pair: (king, in)\n",
      "11. Positive pair: (king, a)\n",
      "12. Positive pair: (king, suite)\n",
      "13. Positive pair: (king, for)\n",
      "14. Positive pair: (suite, a)\n",
      "15. Positive pair: (suite, king)\n",
      "16. Positive pair: (suite, for)\n",
      "17. Positive pair: (suite, [UNK])\n",
      "18. Positive pair: (for, king)\n",
      "19. Positive pair: (for, suite)\n",
      "20. Positive pair: (for, [UNK])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a reverse dictionary for decoding indices to words\n",
    "index_to_word = {index: word for index, word in enumerate(vocabulary)}\n",
    "\n",
    "def load_training_data_parquet(file_path: str):\n",
    "    \"\"\"\n",
    "    Loads pairs and labels from a Parquet file.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: Path to the file containing the training data.\n",
    "\n",
    "    Returns:\n",
    "    - pairs: Numpy array of pairs (target, context/negative).\n",
    "    - labels: Numpy array of labels (1 for positive pairs, 0 for negative pairs).\n",
    "    \"\"\"\n",
    "    table = pq.read_table(file_path)\n",
    "    df = table.to_pandas()\n",
    "    pairs = df[['target_word', 'context_word']].values\n",
    "    labels = df['label'].values\n",
    "    return pairs, labels\n",
    "\n",
    "def print_sample_pairs(pairs, labels, index_to_word, num_samples=5):\n",
    "    \"\"\"\n",
    "    Displays a few word pairs with their decoding and labels.\n",
    "\n",
    "    Parameters:\n",
    "    - pairs: Numpy array of pairs (target, context/negative).\n",
    "    - labels: Numpy array of labels (1 for positive pairs, 0 for negative pairs).\n",
    "    - index_to_word: Reverse dictionary for decoding indices to words.\n",
    "    - num_samples: Number of examples to display.\n",
    "    \"\"\"\n",
    "    print(\"\\nSample pairs with decoding:\")\n",
    "    for i in range(min(num_samples, len(pairs))):\n",
    "        target_idx, context_idx = pairs[i]\n",
    "        target_word = index_to_word.get(target_idx, '[UNK]')\n",
    "        context_word = index_to_word.get(context_idx, '[UNK]')\n",
    "        label = labels[i]\n",
    "        pair_type = \"Positive\" if label == 1 else \"Negative\"\n",
    "        print(f\"{i+1}. {pair_type} pair: ({target_word}, {context_word})\")\n",
    "\n",
    "'''\n",
    "training_data_file = 'training_data.parquet'\n",
    "pairs, labels = load_training_data_parquet(training_data_file)\n",
    "'''\n",
    "print_sample_pairs(pairs, labels, index_to_word, num_samples=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1d455e-f158-4de7-89ce-97d81f4060c0",
   "metadata": {},
   "source": [
    "Example using tensoflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311dda5e-ad94-40b5-a1a5-8bcae9ff991f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62213abc-61eb-45fc-a03d-1e5d91226528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a62272e5-7ddf-4d36-a44c-cc13132be39f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 1\n",
      "Sample pairs (target, context):\n",
      "[(62, array([  63,   68, 1423,    2,   10,   13], dtype=int64))]\n",
      "Sample labels:\n",
      "[[1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "##### understand what is the functionalify of this, before you use it.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "word_to_index = {word: index for index, word in enumerate(vocabulary)}\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "VOCABULARY_SIZE = len(vocabulary)\n",
    "\n",
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(VOCABULARY_SIZE)\n",
    "targets = []\n",
    "contexts = []\n",
    "labels = []\n",
    "\n",
    "NUMBER_OF_NEGATIVE_SAMPLES=5\n",
    "\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "      encoded_dataset[0],\n",
    "      vocabulary_size= VOCABULARY_SIZE,\n",
    "      sampling_table=sampling_table,\n",
    "      window_size=2,\n",
    "      negative_samples=0,\n",
    "      seed=42)\n",
    "\n",
    "\n",
    "# Iterate over each positive skip-gram pair to produce training examples\n",
    "# with a positive context word and negative samples.\n",
    "for target_word, context_word in positive_skip_grams:\n",
    "    \n",
    "    context_class = tf.expand_dims(tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "    negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=NUMBER_OF_NEGATIVE_SAMPLES,\n",
    "          unique=True,\n",
    "          range_max=VOCABULARY_SIZE,\n",
    "          seed=123,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "    # Build context and label vectors (for one target word)\n",
    "    context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
    "    label = tf.constant([1] + [0] * NUMBER_OF_NEGATIVE_SAMPLES, dtype=\"int64\")\n",
    "\n",
    "    # Append each element from the training example to global lists.\n",
    "targets.append(target_word)\n",
    "contexts.append(context)\n",
    "labels.append(label)\n",
    "targets = np.array(targets, dtype=\"int64\")\n",
    "contexts = np.array(contexts, dtype=\"int64\")\n",
    "labels = np.array(labels, dtype=\"int64\")\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(buffer_size=10000).batch(512, drop_remainder=True)\n",
    "\n",
    "\n",
    "print(f\"Number of training examples: {len(labels)}\")\n",
    "print(f\"Sample pairs (target, context):\\n{list(zip(targets, contexts))[:10]}\")\n",
    "print(f\"Sample labels:\\n{labels[:10]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4da797f6-795f-4690-9db4-5438fad9ce94",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected argument value expression (4224374398.py, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[16], line 29\u001b[1;36m\u001b[0m\n\u001b[1;33m    num_sampled=,\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m expected argument value expression\n"
     ]
    }
   ],
   "source": [
    "##### understand what is the functionalify of this, before you use it.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "word_to_index = {word: index for index, word in enumerate(vocabulary)}\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "VOCABULARY_SIZE = len(vocabulary)\n",
    "\n",
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(VOCABULARY_SIZE)\n",
    "\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "      encoded_dataset[0],\n",
    "      vocabulary_size= VOCABULARY_SIZE,\n",
    "      sampling_table=sampling_table,\n",
    "      window_size=2,\n",
    "      negative_samples=0,\n",
    "      seed=42)\n",
    "\n",
    "NUMBER_OF_NEGATIVE_SAMPLES=5\n",
    "# Iterate over each positive skip-gram pair to produce training examples\n",
    "# with a positive context word and negative samples.\n",
    "for target_word, context_word in positive_skip_grams:\n",
    "    \n",
    "    context_class = tf.expand_dims(tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "    negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "      true_classes=context_class,\n",
    "      num_true=1,\n",
    "      num_sampled=,\n",
    "      unique=True,\n",
    "      range_max=VOCABULARY_SIZE,\n",
    "      seed=123,\n",
    "      name=\"negative_sampling\")\n",
    "\n",
    "    # Build context and label vectors (for one target word)\n",
    "    context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
    "    label = tf.constant([1] + [0] * NUMBER_OF_NEGATIVE_SAMPLES, dtype=\"int64\")\n",
    "\n",
    "    # Append each element from the training example to global lists.\n",
    "    targets.append(target_word)\n",
    "    contexts.append(context)\n",
    "    labels.append(label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74188e7-f1cc-42d9-b950-57f2cb469263",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### understand what is the functionalify of this, before you use it.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "word_to_index = {word: index for index, word in enumerate(vocabulary)}\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "VOCABULARY_SIZE = len(vocabulary)\n",
    "\n",
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(VOCABULARY_SIZE)\n",
    "\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "      encoded_dataset[0],\n",
    "      vocabulary_size= VOCABULARY_SIZE,\n",
    "      sampling_table=sampling_table,\n",
    "      window_size=2,\n",
    "      negative_samples=0,\n",
    "      seed=42)\n",
    "\n",
    "NUMBER_OF_NEGATIVE_SAMPLES=5\n",
    "# Iterate over each positive skip-gram pair to produce training examples\n",
    "# with a positive context word and negative samples.\n",
    "for target_word, context_word in positive_skip_grams:\n",
    "    \n",
    "    context_class = tf.expand_dims(tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "    negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "      true_classes=context_class,\n",
    "      num_true=1,\n",
    "      num_sampled=,\n",
    "      unique=True,\n",
    "      range_max=VOCABULARY_SIZE,\n",
    "      seed=123,\n",
    "      name=\"negative_sampling\")\n",
    "\n",
    "    # Build context and label vectors (for one target word)\n",
    "    context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
    "    label = tf.constant([1] + [0] * NUMBER_OF_NEGATIVE_SAMPLES, dtype=\"int64\")\n",
    "\n",
    "    # Append each element from the training example to global lists.\n",
    "    targets.append(target_word)\n",
    "    contexts.append(context)\n",
    "    labels.append(label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c4ab2b-49f3-4a10-9236-c7da3fd79be6",
   "metadata": {},
   "source": [
    "### Define the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a245e822-508a-4491-9de5-31f1f0dbd238",
   "metadata": {},
   "outputs": [],
   "source": [
    " # define you model here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1de30c9-ba08-47aa-8cfb-bd05652d7aa8",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95b979f-9e85-40da-bfc5-86fb90f6e639",
   "metadata": {},
   "source": [
    "Example, there are other ways to train like using gradient tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91359a4b-2f3e-43b6-951c-34fa5c013fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=..., loss=..., metrics=[...])\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"model_logs\")\n",
    "\n",
    "# checkpoint a model. here we save the best model relative to validation loss\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(filepath=\"word2vec_model_w5_ns15_ckpt.h5\", monitor=..., save_best_only=True)\n",
    "\n",
    "# restore_best_weights - Whether to restore model weights from\n",
    "# the epoch with the best value of the monitored quantity.\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor='accuracy')\n",
    "\n",
    "history = model.fit(dataset, epochs=..., callbacks=[tensorboard_callback, checkpoint_cb, early_stopping_cb])\n",
    "\n",
    "model.save(f\"word2vec_model_w{WINDOW_SIZE}_ns{NEGATIVE_SAMPLES}.h5\", include_optimizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084dd0b9-d00b-431f-8c97-7284901421f6",
   "metadata": {},
   "source": [
    "### Save embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46778d77-bc61-4c4f-8d68-dc39650a2f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_embeddings(model, vocabulary: List[str], path: str):\n",
    "    \"\"\" \n",
    "    Save the embeddings \n",
    "    param: model: the trained tf model \n",
    "    param: vocabulary: list of tokens \n",
    "    param: path: the path to save the embeddings map \n",
    "\n",
    "    hint: take the weights using model.get_layer(...).get_weights()\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd18f62-3ec2-4707-98f5-8075e6ec8945",
   "metadata": {},
   "source": [
    "### Find Most similar\n",
    "\n",
    "Note that this must be efficient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466b69bd-718c-41e6-b9a2-3ff8a9b849bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar(word: str, k: int = 10) -> List[Tuple[str, float]]:\n",
    "    \"\"\" \n",
    "    Find most similar tokens to word \n",
    "    param: word: the word to find most similar words to \n",
    "    k: number of most similar words \n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa43790f-8dce-4aea-823d-811485bff72e",
   "metadata": {},
   "source": [
    "### Dimentionality Reduction \n",
    "\n",
    "visualize some clusters (pick some subset of words to show the labels for)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf194e2-78e5-41ee-af40-f7d846786c84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43faf1a7-67f4-4870-801e-7306dab0e57f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2316c74e-f996-4b4a-81dd-f9ae8f710fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca67fe9-c9b3-49d2-a9cb-e0bd117fa020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
