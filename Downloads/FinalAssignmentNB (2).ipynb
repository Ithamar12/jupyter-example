{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f008fdbe-2fe0-48fb-bf66-f6929cec764b",
   "metadata": {},
   "source": [
    "# Final Assignment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a481c7-59ba-475b-9b2c-c9e2bf942f82",
   "metadata": {},
   "source": [
    "## Instuctions\n",
    "* An extra 20 points for those who implement ALL the functions themselves without using tensorflows tf.keras.preprocessing and TextVectorization.\n",
    "* Use pyarrow to save the embedding map.\n",
    "* Find most similar function should be efficient. 20 sec to wait for a result is too much. You should aim to < 2 sec (use \"timeit\" magic to verify)\n",
    "* Output cells should not be too big. DO NOT DUMP A LOT OF DATA IN THE OUTPUT. NOTEOOKS THAT WON'T FOLLOW THIS INSTRUCTION WILL NOT BE CHECKED AND THEIR GRADE WILL BE SET TO ZERO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031f54b0-560c-4d23-95a7-614f71b40334",
   "metadata": {},
   "source": [
    "### Read dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b7098bf-e86b-439d-bf82-ef06c6d55895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in zip: ['review_230k.parquet']\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "zip_path = 'data/review_230k.zip'\n",
    "\n",
    "def extract_parquet_from_zip(zip_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        file_list = zip_ref.namelist()\n",
    "        print(f\"Files in zip: {file_list}\") \n",
    "\n",
    "        parquet_file_name = [f for f in file_list if f.endswith('.parquet')][0]\n",
    "\n",
    "        with zip_ref.open(parquet_file_name) as file:\n",
    "            df = pd.read_parquet(file)\n",
    "    return df\n",
    "\n",
    "data = extract_parquet_from_zip(zip_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d2abf69-dec8-450b-89b5-594a7226cae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data: pd.DataFrame):\n",
    "    \"\"\" Create the dataset in your preferrable format \"\"\"\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58c43b7a-abf6-4ef2-8f04-e3b7d5d32b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                title  \\\n",
      "0             Truly is \"Jewel of the Upper Wets Side\"   \n",
      "1                             My home away from home!   \n",
      "2                                          Great Stay   \n",
      "3                                  Modern Convenience   \n",
      "4       Its the best of the Andaz Brand in the US....   \n",
      "...                                               ...   \n",
      "230334                        Treated us like royalty   \n",
      "230335        Fine time but some room for improvement   \n",
      "230336                Great venue for corporate event   \n",
      "230337                             Almost in the Loop   \n",
      "230338                                    Comfortable   \n",
      "\n",
      "                                                     text  \n",
      "0       Stayed in a king suite for 11 nights and yes i...  \n",
      "1       On every visit to NYC, the Hotel Beacon is the...  \n",
      "2       This is a great property in Midtown. We two di...  \n",
      "3       The Andaz is a nice hotel in a central locatio...  \n",
      "4       I have stayed at each of the US Andaz properti...  \n",
      "...                                                   ...  \n",
      "230334  My kids (17 and 13) stayed at this location De...  \n",
      "230335  Just returned from a fabulous wknd in NYC. The...  \n",
      "230336  I organized a corporate event at this hotel, a...  \n",
      "230337  We had a beautiful room with a balcony and gre...  \n",
      "230338  This hotel is located in Greek Town. Stayed he...  \n",
      "\n",
      "[230339 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset = create_dataset(data)\n",
    "\n",
    "#\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "964d4065-2935-4b64-bff5-a47134baa9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great Stay\n",
      "Stayed in a king suite for 11 nights and yes it cots us a bit but we were happy with the standard of room, the location and the friendliness of the staff. Our room was on the 20th floor overlooking Broadway and the madhouse of the Fairway Market. Room was quite with no noise evident from the hallway or adjoining rooms. It was great to be able to open windows when we craved fresh rather than heated air. The beds, including the fold out sofa bed, were comfortable and the rooms were cleaned well. Wi-fi access worked like a dream with only one connectivity issue on our first night and this was promptly responded to with a call from the service provider to ensure that all was well. The location close to the 72nd Street subway station is great and the complimentary umbrellas on the drizzly days were greatly appreciated. It is fabulous to have the kitchen with cooking facilities and the access to a whole range of fresh foods directly across the road at Fairway.\n",
      "This is the second time that members of the party have stayed at the Beacon and it will certainly be our hotel of choice for future visits.\n"
     ]
    }
   ],
   "source": [
    "first_title_value = dataset['text'].iloc[1]\n",
    "print(dataset['title'].iloc[2])\n",
    "print(dataset['text'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c4e719-a0c9-4ce3-8feb-e0ec551b4c76",
   "metadata": {},
   "source": [
    "### Clean and standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbdb4643-5bde-4356-8576-3077c66ea0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stayed', 'in', 'a', 'king', 'suite', 'for', 'nights', 'and', 'yes', 'it', 'cots', 'us', 'a', 'bit', 'but', 'we', 'were', 'happy', 'with', 'the', 'standard', 'of', 'room', 'the', 'location', 'and', 'the', 'friendliness', 'of', 'the', 'staff']\n",
      "['our', 'room', 'was', 'on', 'the', 'th', 'floor', 'overlooking', 'broadway', 'and', 'the', 'madhouse', 'of', 'the', 'fairway', 'market']\n",
      "['room', 'was', 'quite', 'with', 'no', 'noise', 'evident', 'from', 'the', 'hallway', 'or', 'adjoining', 'rooms']\n",
      "['it', 'was', 'great', 'to', 'be', 'able', 'to', 'open', 'windows', 'when', 'we', 'craved', 'fresh', 'rather', 'than', 'heated', 'air']\n",
      "['the', 'beds', 'including', 'the', 'fold', 'out', 'sofa', 'bed', 'were', 'comfortable', 'and', 'the', 'rooms', 'were', 'cleaned', 'well']\n",
      "['wifi', 'access', 'worked', 'like', 'a', 'dream', 'with', 'only', 'one', 'connectivity', 'issue', 'on', 'our', 'first', 'night', 'and', 'this', 'was', 'promptly', 'responded', 'to', 'with', 'a', 'call', 'from', 'the', 'service', 'provider', 'to', 'ensure', 'that', 'all', 'was', 'well']\n",
      "['the', 'location', 'close', 'to', 'the', 'nd', 'street', 'subway', 'station', 'is', 'great', 'and', 'the', 'complimentary', 'umbrellas', 'on', 'the', 'drizzly', 'days', 'were', 'greatly', 'appreciated']\n",
      "['it', 'is', 'fabulous', 'to', 'have', 'the', 'kitchen', 'with', 'cooking', 'facilities', 'and', 'the', 'access', 'to', 'a', 'whole', 'range', 'of', 'fresh', 'foods', 'directly', 'across', 'the', 'road', 'at', 'fairway']\n",
      "['this', 'is', 'the', 'second', 'time', 'that', 'members', 'of', 'the', 'party', 'have', 'stayed', 'at', 'the', 'beacon', 'and', 'it', 'will', 'certainly', 'be', 'our', 'hotel', 'of', 'choice', 'for', 'future', 'visits']\n",
      "['on', 'every', 'visit', 'to', 'nyc', 'the', 'hotel', 'beacon', 'is', 'the', 'place', 'we', 'love', 'to', 'stay']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import string\n",
    "\n",
    "def prepare_dataset(dataset):\n",
    "    \"\"\" Clean and prepare the dataset before word encoding, returning a single 2D array \"\"\"\n",
    "    \n",
    "    dataset['text'] = dataset['text'].fillna('')\n",
    "    \n",
    "    def clean_text(text):\n",
    "        text = text.lower()\n",
    "        sentences = text.split('.')\n",
    "        cleaned_sentences = [\n",
    "            sentence.translate(str.maketrans('', '', string.punctuation + string.digits)).strip()\n",
    "            for sentence in sentences if sentence\n",
    "        ]\n",
    "        \n",
    "        sentence_word_arrays = [sentence.split() for sentence in cleaned_sentences]\n",
    "        return sentence_word_arrays\n",
    "    dataset['tokenized_sentences'] = dataset['text'].apply(clean_text)\n",
    "    all_tokenized_sentences = []\n",
    "    for sentence_list in dataset['tokenized_sentences']:\n",
    "        all_tokenized_sentences.extend(sentence_list)\n",
    "    \n",
    "    return all_tokenized_sentences\n",
    "\n",
    "all_tokenized_sentences = prepare_dataset(dataset)\n",
    "\n",
    "for sentence in all_tokenized_sentences[:10]:\n",
    "    print(sentence) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d4fd16-3787-411e-b098-32a1de657f71",
   "metadata": {},
   "source": [
    "### Create A vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a56e5ae3-712a-4a30-8b50-a7f5ac3cf684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word_count_map = Counter()\n",
    "\n",
    "for sentence in all_tokenized_sentences:\n",
    "    word_count_map.update(sentence)\n",
    "vocabulary = {word: idx for idx, word in enumerate(word_count_map.keys())}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfa0b49-60b8-407e-9d6b-c5eef5f2e28e",
   "metadata": {},
   "source": [
    "### Word encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40e66a17-f60e-4686-a360-2e4a483ed2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 2, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 18, 22, 7, 18, 23, 20, 18, 24]\n",
      "[25, 21, 26, 27, 18, 28, 29, 30, 31, 7, 18, 32, 20, 18, 33, 34]\n",
      "[21, 26, 35, 17, 36, 37, 38, 39, 18, 40, 41, 42, 43]\n",
      "[9, 26, 44, 45, 46, 47, 45, 48, 49, 50, 14, 51, 52, 53, 54, 55, 56]\n",
      "[18, 57, 58, 18, 59, 60, 61, 62, 15, 63, 7, 18, 43, 15, 64, 65]\n",
      "[66, 67, 68, 69, 2, 70, 17, 71, 72, 73, 74, 27, 25, 75, 76, 7, 77, 26, 78, 79, 45, 17, 2, 80, 39, 18, 81, 82, 45, 83, 84, 85, 26, 65]\n",
      "[18, 22, 86, 45, 18, 87, 88, 89, 90, 91, 44, 7, 18, 92, 93, 27, 18, 94, 95, 15, 96, 97]\n",
      "[9, 91, 98, 45, 99, 18, 100, 17, 101, 102, 7, 18, 67, 45, 2, 103, 104, 20, 52, 105, 106, 107, 18, 108, 109, 33]\n",
      "[77, 91, 18, 110, 111, 84, 112, 20, 18, 113, 99, 0, 109, 18, 114, 7, 9, 115, 116, 46, 25, 117, 20, 118, 5, 119, 120]\n",
      "[27, 121, 122, 45, 123, 18, 117, 114, 91, 18, 124, 14, 125, 45, 126]\n"
     ]
    }
   ],
   "source": [
    "def encode_dataset(dataset, vocabulary: dict[str, int]):\n",
    "    \"\"\" Encode each word in the dataset based on the given vocabulary \"\"\"\n",
    "    encoded_dataset = []\n",
    "    for sentence in dataset:\n",
    "        encoded_sentence = [vocabulary[word] for word in sentence if word in vocabulary]\n",
    "        encoded_dataset.append(encoded_sentence)\n",
    "    \n",
    "    return encoded_dataset\n",
    "\n",
    "encoded_dataset = encode_dataset(all_tokenized_sentences, vocabulary)\n",
    "\n",
    "for sentence in encoded_dataset[:10]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a41657-004a-4a20-9e11-f055656760ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fb906ef-e136-4b0a-a083-68370c27b530",
   "metadata": {},
   "source": [
    "Example using tensoflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3a1208a-6ac2-4c39-a7c6-23338bc961f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_vocabulary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# you can also use \"adapt\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m vocabulary \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_vocabulary\u001b[49m(dataset) \n\u001b[0;32m      6\u001b[0m vectorize_layer \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mTextVectorization(\n\u001b[0;32m      7\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,\n\u001b[0;32m      8\u001b[0m     output_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      9\u001b[0m     output_sequence_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,\n\u001b[0;32m     10\u001b[0m     vocabulary\u001b[38;5;241m=\u001b[39mvocabulary\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# adds padding and [UNK] token\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_vocabulary' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# you can also use \"adapt\n",
    "vocabulary = create_vocabulary(dataset) \n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    max_tokens=...,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=...,\n",
    "    vocabulary=vocabulary\n",
    ")\n",
    "\n",
    "# adds padding and [UNK] token\n",
    "vocabulary = vectorize_layer.get_vocabulary()\n",
    "\n",
    "encoded_text_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n",
    "encoded_dataset = encoded_text_ds.as_numpy_iterator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af56a63-d20b-4292-8edf-95f27b029186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d1cb0a0-78a4-469a-8081-f325d97c8974",
   "metadata": {},
   "source": [
    "### Generate Positive and Negative Pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744b28fc-753a-420c-8e1b-3346e98c0c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(encoded_dataset, number_of_neg_samples: int):\n",
    "    \"\"\" \n",
    "    Generate positive and negative pairs \n",
    "    param: encoded_dataset:  the encoded dataset\n",
    "    param: number_of_neg_samples: Number of negative samples per positive pair \n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1d455e-f158-4de7-89ce-97d81f4060c0",
   "metadata": {},
   "source": [
    "Example using tensoflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c85fda-64c4-438c-b78b-14ee296b9602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# understand what is the functionalify of this, before you use it.\n",
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(len(vocabulary))\n",
    "\n",
    "\n",
    "# Generate positive skip-gram pairs for a sequence\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "      sequence,\n",
    "      vocabulary_size=VOCABULARY_SIZE,\n",
    "      sampling_table=sampling_table,\n",
    "      window_size=...,\n",
    "      negative_samples=0)\n",
    "\n",
    "\n",
    "# Iterate over each positive skip-gram pair to produce training examples\n",
    "# with a positive context word and negative samples.\n",
    "for target_word, context_word in positive_skip_grams:\n",
    "    \n",
    "    context_class = tf.expand_dims(tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "    negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "      true_classes=context_class,\n",
    "      num_true=1,\n",
    "      num_sampled=NUMBER_OF_NEGATIVE_SAMPLES,\n",
    "      unique=True,\n",
    "      range_max=VOCABULARY_SIZE,\n",
    "      seed=123,\n",
    "      name=\"negative_sampling\")\n",
    "\n",
    "    # Build context and label vectors (for one target word)\n",
    "    context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
    "    label = tf.constant([1] + [0] * NUMBER_OF_NEGATIVE_SAMPLES, dtype=\"int64\")\n",
    "\n",
    "    # Append each element from the training example to global lists.\n",
    "    targets.append(target_word)\n",
    "    contexts.append(context)\n",
    "    labels.append(label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c4ab2b-49f3-4a10-9236-c7da3fd79be6",
   "metadata": {},
   "source": [
    "### Define the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a245e822-508a-4491-9de5-31f1f0dbd238",
   "metadata": {},
   "outputs": [],
   "source": [
    " # define you model here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1de30c9-ba08-47aa-8cfb-bd05652d7aa8",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95b979f-9e85-40da-bfc5-86fb90f6e639",
   "metadata": {},
   "source": [
    "Example, there are other ways to train like using gradient tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91359a4b-2f3e-43b6-951c-34fa5c013fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=..., loss=..., metrics=[...])\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"model_logs\")\n",
    "\n",
    "# checkpoint a model. here we save the best model relative to validation loss\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(filepath=\"word2vec_model_w5_ns15_ckpt.h5\", monitor=..., save_best_only=True)\n",
    "\n",
    "# restore_best_weights - Whether to restore model weights from\n",
    "# the epoch with the best value of the monitored quantity.\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor='accuracy')\n",
    "\n",
    "history = model.fit(dataset, epochs=..., callbacks=[tensorboard_callback, checkpoint_cb, early_stopping_cb])\n",
    "\n",
    "model.save(f\"word2vec_model_w{WINDOW_SIZE}_ns{NEGATIVE_SAMPLES}.h5\", include_optimizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084dd0b9-d00b-431f-8c97-7284901421f6",
   "metadata": {},
   "source": [
    "### Save embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46778d77-bc61-4c4f-8d68-dc39650a2f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_embeddings(model, vocabulary: List[str], path: str):\n",
    "    \"\"\" \n",
    "    Save the embeddings \n",
    "    param: model: the trained tf model \n",
    "    param: vocabulary: list of tokens \n",
    "    param: path: the path to save the embeddings map \n",
    "\n",
    "    hint: take the weights using model.get_layer(...).get_weights()\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd18f62-3ec2-4707-98f5-8075e6ec8945",
   "metadata": {},
   "source": [
    "### Find Most similar\n",
    "\n",
    "Note that this must be efficient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466b69bd-718c-41e6-b9a2-3ff8a9b849bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar(word: str, k: int = 10) -> List[Tuple[str, float]]:\n",
    "    \"\"\" \n",
    "    Find most similar tokens to word \n",
    "    param: word: the word to find most similar words to \n",
    "    k: number of most similar words \n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa43790f-8dce-4aea-823d-811485bff72e",
   "metadata": {},
   "source": [
    "### Dimentionality Reduction \n",
    "\n",
    "visualize some clusters (pick some subset of words to show the labels for)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf194e2-78e5-41ee-af40-f7d846786c84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43faf1a7-67f4-4870-801e-7306dab0e57f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2316c74e-f996-4b4a-81dd-f9ae8f710fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca67fe9-c9b3-49d2-a9cb-e0bd117fa020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
