{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f008fdbe-2fe0-48fb-bf66-f6929cec764b",
   "metadata": {},
   "source": [
    "# Final Assignment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a481c7-59ba-475b-9b2c-c9e2bf942f82",
   "metadata": {},
   "source": [
    "## Instuctions\n",
    "* An extra 20 points for those who implement ALL the functions themselves without using tensorflows tf.keras.preprocessing and TextVectorization.\n",
    "* Use pyarrow to save the embedding map.\n",
    "* Find most similar function should be efficient. 20 sec to wait for a result is too much. You should aim to < 2 sec (use \"timeit\" magic to verify)\n",
    "* Output cells should not be too big. DO NOT DUMP A LOT OF DATA IN THE OUTPUT. NOTEOOKS THAT WON'T FOLLOW THIS INSTRUCTION WILL NOT BE CHECKED AND THEIR GRADE WILL BE SET TO ZERO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031f54b0-560c-4d23-95a7-614f71b40334",
   "metadata": {},
   "source": [
    "### Read dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1685c817-ef23-48f9-b01e-429d8b5b4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "path_to_data = \"\"\n",
    "data = pd.read_parquet(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2abf69-dec8-450b-89b5-594a7226cae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data: pd.DataFrame):\n",
    "    \"\"\" Create the dataset in your preferrable format \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c43b7a-abf6-4ef2-8f04-e3b7d5d32b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c4e719-a0c9-4ce3-8feb-e0ec551b4c76",
   "metadata": {},
   "source": [
    "### Clean and standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdb4643-5bde-4356-8576-3077c66ea0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset):\n",
    "    \"\"\" Clean and prepare the dataset before word encoding \"\"\"\n",
    "    pass \n",
    "\n",
    "dataset = prepare_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d4fd16-3787-411e-b098-32a1de657f71",
   "metadata": {},
   "source": [
    "### Create A vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56e5ae3-712a-4a30-8b50-a7f5ac3cf684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(dataset) -> dict[str, int]:\n",
    "    \"\"\" Create a dictionary mapping each word to its respective frequency \"\"\"\n",
    "    pass \n",
    "\n",
    "vocabulary = create_vocabulary(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfa0b49-60b8-407e-9d6b-c5eef5f2e28e",
   "metadata": {},
   "source": [
    "### Word encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e66a17-f60e-4686-a360-2e4a483ed2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataset(dataset, vocabulary: dict[str, int]):\n",
    "    \"\"\" Encode each word in the dataset, based on your vocabulary \"\"\"\n",
    "    pass\n",
    "\n",
    "encoded_dataset = encode_dataset(dataset, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a41657-004a-4a20-9e11-f055656760ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fb906ef-e136-4b0a-a083-68370c27b530",
   "metadata": {},
   "source": [
    "Example using tensoflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a1208a-6ac2-4c39-a7c6-23338bc961f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# you can also use \"adapt\" \n",
    "vocabulary = create_vocabulary(dataset) \n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    max_tokens=...,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=...,\n",
    "    vocabulary=vocabulary\n",
    ")\n",
    "\n",
    "# adds padding and [UNK] token\n",
    "vocabulary = vectorize_layer.get_vocabulary()\n",
    "\n",
    "encoded_text_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n",
    "encoded_dataset = encoded_text_ds.as_numpy_iterator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af56a63-d20b-4292-8edf-95f27b029186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d1cb0a0-78a4-469a-8081-f325d97c8974",
   "metadata": {},
   "source": [
    "### Generate Positive and Negative Pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744b28fc-753a-420c-8e1b-3346e98c0c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(encoded_dataset, number_of_neg_samples: int):\n",
    "    \"\"\" \n",
    "    Generate positive and negative pairs \n",
    "    param: encoded_dataset:  the encoded dataset\n",
    "    param: number_of_neg_samples: Number of negative samples per positive pair \n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1d455e-f158-4de7-89ce-97d81f4060c0",
   "metadata": {},
   "source": [
    "Example using tensoflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c85fda-64c4-438c-b78b-14ee296b9602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# understand what is the functionalify of this, before you use it.\n",
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(len(vocabulary))\n",
    "\n",
    "\n",
    "# Generate positive skip-gram pairs for a sequence\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "      sequence,\n",
    "      vocabulary_size=VOCABULARY_SIZE,\n",
    "      sampling_table=sampling_table,\n",
    "      window_size=...,\n",
    "      negative_samples=0)\n",
    "\n",
    "\n",
    "# Iterate over each positive skip-gram pair to produce training examples\n",
    "# with a positive context word and negative samples.\n",
    "for target_word, context_word in positive_skip_grams:\n",
    "    \n",
    "    context_class = tf.expand_dims(tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "    negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "      true_classes=context_class,\n",
    "      num_true=1,\n",
    "      num_sampled=NUMBER_OF_NEGATIVE_SAMPLES,\n",
    "      unique=True,\n",
    "      range_max=VOCABULARY_SIZE,\n",
    "      seed=123,\n",
    "      name=\"negative_sampling\")\n",
    "\n",
    "    # Build context and label vectors (for one target word)\n",
    "    context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
    "    label = tf.constant([1] + [0] * NUMBER_OF_NEGATIVE_SAMPLES, dtype=\"int64\")\n",
    "\n",
    "    # Append each element from the training example to global lists.\n",
    "    targets.append(target_word)\n",
    "    contexts.append(context)\n",
    "    labels.append(label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c4ab2b-49f3-4a10-9236-c7da3fd79be6",
   "metadata": {},
   "source": [
    "### Define the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a245e822-508a-4491-9de5-31f1f0dbd238",
   "metadata": {},
   "outputs": [],
   "source": [
    " # define you model here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1de30c9-ba08-47aa-8cfb-bd05652d7aa8",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95b979f-9e85-40da-bfc5-86fb90f6e639",
   "metadata": {},
   "source": [
    "Example, there are other ways to train like using gradient tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91359a4b-2f3e-43b6-951c-34fa5c013fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=..., loss=..., metrics=[...])\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"model_logs\")\n",
    "\n",
    "# checkpoint a model. here we save the best model relative to validation loss\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(filepath=\"word2vec_model_w5_ns15_ckpt.h5\", monitor=..., save_best_only=True)\n",
    "\n",
    "# restore_best_weights - Whether to restore model weights from\n",
    "# the epoch with the best value of the monitored quantity.\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor='accuracy')\n",
    "\n",
    "history = model.fit(dataset, epochs=..., callbacks=[tensorboard_callback, checkpoint_cb, early_stopping_cb])\n",
    "\n",
    "model.save(f\"word2vec_model_w{WINDOW_SIZE}_ns{NEGATIVE_SAMPLES}.h5\", include_optimizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084dd0b9-d00b-431f-8c97-7284901421f6",
   "metadata": {},
   "source": [
    "### Save embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46778d77-bc61-4c4f-8d68-dc39650a2f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_embeddings(model, vocabulary: List[str], path: str):\n",
    "    \"\"\" \n",
    "    Save the embeddings \n",
    "    param: model: the trained tf model \n",
    "    param: vocabulary: list of tokens \n",
    "    param: path: the path to save the embeddings map \n",
    "\n",
    "    hint: take the weights using model.get_layer(...).get_weights()\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd18f62-3ec2-4707-98f5-8075e6ec8945",
   "metadata": {},
   "source": [
    "### Find Most similar\n",
    "\n",
    "Note that this must be efficient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466b69bd-718c-41e6-b9a2-3ff8a9b849bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar(word: str, k: int = 10) -> List[Tuple[str, float]]:\n",
    "    \"\"\" \n",
    "    Find most similar tokens to word \n",
    "    param: word: the word to find most similar words to \n",
    "    k: number of most similar words \n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa43790f-8dce-4aea-823d-811485bff72e",
   "metadata": {},
   "source": [
    "### Dimentionality Reduction \n",
    "\n",
    "visualize some clusters (pick some subset of words to show the labels for)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf194e2-78e5-41ee-af40-f7d846786c84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43faf1a7-67f4-4870-801e-7306dab0e57f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
