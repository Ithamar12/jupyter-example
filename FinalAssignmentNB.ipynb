{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f008fdbe-2fe0-48fb-bf66-f6929cec764b",
   "metadata": {},
   "source": [
    "# Final Assignment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a481c7-59ba-475b-9b2c-c9e2bf942f82",
   "metadata": {},
   "source": [
    "## Instuctions\n",
    "* An extra 20 points for those who implement ALL the functions themselves without using tensorflows tf.keras.preprocessing and TextVectorization.\n",
    "* Use pyarrow to save the embedding map.\n",
    "* Find most similar function should be efficient. 20 sec to wait for a result is too much. You should aim to < 2 sec (use \"timeit\" magic to verify)\n",
    "* Output cells should not be too big. DO NOT DUMP A LOT OF DATA IN THE OUTPUT. NOTEOOKS THAT WON'T FOLLOW THIS INSTRUCTION WILL NOT BE CHECKED AND THEIR GRADE WILL BE SET TO ZERO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031f54b0-560c-4d23-95a7-614f71b40334",
   "metadata": {},
   "source": [
    "### Read dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b7098bf-e86b-439d-bf82-ef06c6d55895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in zip: ['review_230k.parquet']\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "zip_path = 'data/review_230k.zip'\n",
    "\n",
    "def extract_parquet_from_zip(zip_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        file_list = zip_ref.namelist()\n",
    "        print(f\"Files in zip: {file_list}\") \n",
    "\n",
    "        parquet_file_name = [f for f in file_list if f.endswith('.parquet')][0]\n",
    "\n",
    "        with zip_ref.open(parquet_file_name) as file:\n",
    "            df = pd.read_parquet(file)\n",
    "    return df\n",
    "\n",
    "data = extract_parquet_from_zip(zip_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d2abf69-dec8-450b-89b5-594a7226cae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data: pd.DataFrame):\n",
    "    \"\"\" Create the dataset in your preferrable format \"\"\"\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58c43b7a-abf6-4ef2-8f04-e3b7d5d32b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           title  \\\n",
      "0        Truly is \"Jewel of the Upper Wets Side\"   \n",
      "1                        My home away from home!   \n",
      "2                                     Great Stay   \n",
      "3                             Modern Convenience   \n",
      "4  Its the best of the Andaz Brand in the US....   \n",
      "\n",
      "                                                text  \n",
      "0  Stayed in a king suite for 11 nights and yes i...  \n",
      "1  On every visit to NYC, the Hotel Beacon is the...  \n",
      "2  This is a great property in Midtown. We two di...  \n",
      "3  The Andaz is a nice hotel in a central locatio...  \n",
      "4  I have stayed at each of the US Andaz properti...  \n"
     ]
    }
   ],
   "source": [
    "dataset = create_dataset(data)\n",
    "\n",
    "#\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c4e719-a0c9-4ce3-8feb-e0ec551b4c76",
   "metadata": {},
   "source": [
    "### Clean and standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbdb4643-5bde-4356-8576-3077c66ea0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['truly', 'is', 'jewel', 'of', 'the', 'upper', 'wets', 'side', 'stayed', 'in', 'a', 'king', 'suite', 'for', 'nights', 'and', 'yes', 'it', 'cots', 'us', 'a', 'bit', 'but', 'we', 'were', 'happy', 'with', 'the', 'standard', 'of', 'room', 'the', 'location', 'and', 'the', 'friendliness', 'of', 'the', 'staff', 'our', 'room', 'was', 'on', 'the', 'th', 'floor', 'overlooking', 'broadway', 'and', 'the', 'madhouse', 'of', 'the', 'fairway', 'market', 'room', 'was', 'quite', 'with', 'no', 'noise', 'evident', 'from', 'the', 'hallway', 'or', 'adjoining', 'rooms', 'it', 'was', 'great', 'to', 'be', 'able', 'to', 'open', 'windows', 'when', 'we', 'craved', 'fresh', 'rather', 'than', 'heated', 'air', 'the', 'beds', 'including', 'the', 'fold', 'out', 'sofa', 'bed', 'were', 'comfortable', 'and', 'the', 'rooms', 'were', 'cleaned', 'well', 'wifi', 'access', 'worked', 'like', 'a', 'dream', 'with', 'only', 'one', 'connectivity', 'issue', 'on', 'our', 'first', 'night', 'and', 'this', 'was', 'promptly', 'responded', 'to', 'with', 'a', 'call', 'from', 'the', 'service', 'provider', 'to', 'ensure', 'that', 'all', 'was', 'well', 'the', 'location', 'close', 'to', 'the', 'nd', 'street', 'subway', 'station', 'is', 'great', 'and', 'the', 'complimentary', 'umbrellas', 'on', 'the', 'drizzly', 'days', 'were', 'greatly', 'appreciated', 'it', 'is', 'fabulous', 'to', 'have', 'the', 'kitchen', 'with', 'cooking', 'facilities', 'and', 'the', 'access', 'to', 'a', 'whole', 'range', 'of', 'fresh', 'foods', 'directly', 'across', 'the', 'road', 'at', 'fairway', 'this', 'is', 'the', 'second', 'time', 'that', 'members', 'of', 'the', 'party', 'have', 'stayed', 'at', 'the', 'beacon', 'and', 'it', 'will', 'certainly', 'be', 'our', 'hotel', 'of', 'choice', 'for', 'future', 'visits']]\n",
      "[['my', 'home', 'away', 'from', 'home', 'on', 'every', 'visit', 'to', 'nyc', 'the', 'hotel', 'beacon', 'is', 'the', 'place', 'we', 'love', 'to', 'stay', 'so', 'conveniently', 'located', 'to', 'central', 'park', 'lincoln', 'center', 'and', 'great', 'local', 'restaurants', 'the', 'rooms', 'are', 'lovely', 'beds', 'so', 'comfortable', 'a', 'great', 'little', 'kitchen', 'and', 'new', 'wizz', 'bang', 'coffee', 'maker', 'the', 'staff', 'are', 'so', 'accommodating', 'and', 'just', 'love', 'walking', 'across', 'the', 'street', 'to', 'the', 'fairway', 'supermarket', 'with', 'every', 'imaginable', 'goodies', 'to', 'eat', 'if', 'you', 'choose', 'not', 'to', 'go', 'out', 'for', 'every', 'meal']]\n",
      "[['great', 'stay', 'this', 'is', 'a', 'great', 'property', 'in', 'midtown', 'we', 'two', 'different', 'rooms', 'different', 'rooms', 'during', 'our', 'stay', 'the', 'first', 'room', 'was', 'in', 'the', 'north', 'tower', 'which', 'was', 'quite', 'inconvenient', 'you', 'have', 'to', 'go', 'through', 'the', 'conference', 'area', 'to', 'get', 'to', 'the', 'north', 'elevators', 'the', 'second', 'room', 'was', 'the', 'andaz', 'suite', 'it', 'was', 'nicely', 'appointed', 'room', 'but', 'the', 'best', 'part', 'about', 'it', 'was', 'the', 'bathroom', 'from', 'the', 'foot', 'soaking', 'bowl', 'to', 'the', 'bath', 'products', 'everything', 'about', 'the', 'bathroom', 'was', 'awesome', 'lemon', 'poppyseed', 'pancakes', 'are', 'must', 'haves', 'at', 'the', 'restaurant', 'one', 'of', 'the', 'best', 'pancakes', 'ever']]\n",
      "[['modern', 'convenience', 'the', 'andaz', 'is', 'a', 'nice', 'hotel', 'in', 'a', 'central', 'location', 'of', 'manhattan', 'the', 'hyatt', 'has', 'come', 'up', 'with', 'a', 'modern', 'hotel', 'that', 'is', 'both', 'comfortable', 'and', 'convenient', 'when', 'you', 'arrive', 'you', 'are', 'greeted', 'by', 'friendly', 'hosts', 'and', 'they', 'walk', 'you', 'to', 'the', 'checkin', 'desk', 'while', 'offering', 'you', 'a', 'beverage', 'we', 'had', 'a', 'one', 'bedroom', 'suite', 'that', 'accommodated', 'four', 'people', 'reasonably', 'well', 'plenty', 'of', 'closet', 'space', 'well', 'lit', 'with', 'floor', 'to', 'ceiling', 'windows', 'and', 'actually', 'quiet', 'the', 'bathroom', 'was', 'large', 'with', 'a', 'very', 'nice', 'walkin', 'shower', 'and', 'a', 'builtin', 'bench', 'with', 'unique', 'low', 'spout', 'to', 'wash', 'your', 'feet', 'the', 'kitchenette', 'was', 'a', 'nice', 'touch', 'with', 'a', 'stocked', 'fridge', 'offering', 'complimentary', 'nonalcoholic', 'beverages', 'and', 'snacks', 'dishes', 'and', 'utensils', 'a', 'sink', 'dishwasher', 'and', 'a', 'microwave', 'they', 'have', 'daily', 'happy', 'hours', 'where', 'you', 'can', 'get', 'a', 'complimentary', 'decent', 'glass', 'of', 'wine', 'in', 'the', 'modest', 'lobby', 'lounge', 'and', 'bring', 'it', 'to', 'your', 'room', 'the', 'lobby', 'lounge', 'has', 'some', 'seating', 'and', 'one', 'table', 'with', 'or', 'so', 'chairs', 'where', 'you', 'can', 'buy', 'food', 'from', 'the', 'adjacent', 'restaurant', 'one', 'suggestion', 'is', 'to', 'offer', 'selections', 'of', 'cheese', 'and', 'crackers', 'platters', 'to', 'go', 'with', 'that', 'wine', 'we', 'ordered', 'one', 'that', 'had', 'to', 'be', 'custom', 'made', 'but', 'it', 'worked', 'well', 'we', 'didnt', 'eat', 'in', 'the', 'hotel', 'restaurants', 'the', 'restaurant', 'off', 'the', 'lobby', 'was', 'very', 'pricey', 'for', 'breakfast', 'per', 'entree', 'when', 'you', 'can', 'get', 'a', 'decent', 'breakfast', 'within', 'a', 'block', 'or', 'two', 'of', 'the', 'hotel', 'for', 'under', 'what', 'can', 'i', 'say', 'as', 'a', 'hotel', 'designer', 'and', 'someone', 'who', 'is', 'very', 'picky', 'about', 'hotels', 'i', 'will', 'definitely', 'stay', 'there', 'again', 'and', 'i', 'highly', 'recommend', 'it']]\n",
      "[['its', 'the', 'best', 'of', 'the', 'andaz', 'brand', 'in', 'the', 'us', 'i', 'have', 'stayed', 'at', 'each', 'of', 'the', 'us', 'andaz', 'properties', 'and', 'this', 'one', 'is', 'the', 'best', 'of', 'them', 'all', 'much', 'better', 'than', 'the', 'west', 'hollywood', 'property', 'where', 'brand', 'standards', 'are', 'slowly', 'fading', 'this', 'is', 'the', 'shining', 'star', 'here', 'in', 'the', 'states', 'my', 'room', 'suite', 'on', 'the', 'th', 'floor', 'was', 'fantastic', 'as', 'usual', 'perfectly', 'maintained', 'very', 'comfortable', 'great', 'bathrooms', 'best', 'in', 'nyc', 'the', 'bathroom', 'is', 'huge', 'with', 'large', 'shower', 'room', 'and', 'multiple', 'shower', 'heads', 'double', 'lavatories', 'flank', 'one', 'wall', 'huge', 'the', 'bed', 'is', 'always', 'perfect', 'televisions', 'in', 'both', 'rooms', 'were', 'perfect', 'for', 'my', 'limited', 'time', 'spend', 'in', 'the', 'suite', 'nice', 'desk', 'for', 'work', 'area', 'and', 'in', 'room', 'dining', 'every', 'room', 'has', 'a', 'fridge', 'with', 'complimentary', 'soft', 'drinks', 'snacks', 'water', 'sparkling', 'water', 'milk', 'and', 'juices', 'nice', 'touch', 'suite', 'has', 'full', 'size', 'refrigerator', 'with', 'icemaker', 'and', 'also', 'features', 'a', 'microwave', 'sink', 'and', 'dishwasher', 'drawerall', 'nicely', 'concealed', 'behind', 'beautiful', 'cabinetry', 'really', 'great', 'for', 'a', 'longer', 'stay', 'i', 'do', 'have', 'complaints', 'first', 'the', 'elevators', 'the', 'waits', 'were', 'sometimes', 'ridiculous', 'elevators', 'serve', 'the', 'main', 'tower', 'with', 'a', 'rather', 'limited', 'number', 'of', 'rooms', 'this', 'is', 'a', 'newer', 'hotel', 'but', 'there', 'is', 'no', 'service', 'elevator', 'i', 'was', 'continually', 'surprised', 'after', 'waiting', 'for', 'an', 'elevator', 'at', 'how', 'many', 'stops', 'we', 'made', 'for', 'housekeeping', 'to', 'enter', 'with', 'carts', 'room', 'service', 'carts', 'and', 'trays', 'etc', 'how', 'could', 'hyatt', 'skip', 'a', 'service', 'elevator', 'on', 'this', 'new', 'property', 'it', 'really', 'stinks', 'at', 'timesits', 'bad', 'lastly', 'i', 'have', 'traveled', 'with', 'world', 'and', 'certainly', 'know', 'nyc', 'pricing', 'breakfast', 'at', 'the', 'andaz', 'ranks', 'above', 'almost', 'anything', 'i', 'have', 'ever', 'paid', 'apparently', 'i', 'had', 'a', 'breakfast', 'for', 'two', 'the', 'first', 'morning', 'and', 'the', 'second', 'eggs', 'for', 'each', 'coffee', 'berries', 'wow', 'of', 'course', 'i', 'really', 'dont', 'knowas', 'there', 'is', 'no', 'bill', 'they', 'just', 'deliver', 'it', 'and', 'leave', 'prices', 'are', 'listed', 'in', 'the', 'insuite', 'menu', 'but', 'no', 'itemized', 'billing', 'is', 'given', 'i', 'am', 'gifted', 'with', 'a', 'credit', 'for', 'my', 'breakfast', 'as', 'a', 'diamond', 'member', 'and', 'i', 'assumed', 'that', 'had', 'not', 'been', 'applied', 'when', 'my', 'bill', 'reflected', 'a', 'non', 'itemized', 'charge', 'for', 'plus', 'for', 'the', 'first', 'night', 'and', 'slightly', 'less', 'for', 'the', 'second', 'i', 'was', 'wrong', 'it', 'had', 'been', 'applied', 'no', 'way', 'for', 'me', 'to', 'see', 'this', 'wow', 'it', 'certainly', 'isnt', 'worth', 'it', 'i', 'know', 'i', 'sound', 'like', 'one', 'of', 'those', 'guys', 'who', 'complains', 'about', 'pricing', 'but', 'clearly', 'doesnt', 'know', 'what', 'things', 'cost', 'wrong', 'this', 'one', 'really', 'surprised', 'me', 'and', 'i', 'have', 'traveled', 'the', 'world', 'for', 'more', 'than', 'years', 'staying', 'in', 'some', 'of', 'the', 'finest', 'hotels', 'in', 'the', 'world', 'andaz', 'is', 'great', 'but', 'it', 'doesnt', 'rank', 'with', 'many', 'that', 'i', 'have', 'had', 'the', 'pleasure', 'of', 'staying', 'with', 'i', 'just', 'had', 'breakfast', 'this', 'morning', 'for', 'at', 'the', 'four', 'seasons', 'nyc', 'it', 'was', 'half', 'the', 'costalmost', 'stay', 'away', 'from', 'room', 'service', 'in', 'my', 'opinion']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import string\n",
    "\n",
    "def prepare_dataset(dataset: pd.DataFrame):\n",
    "  \n",
    "    dataset['combined_text'] = dataset['title'] + ' ' + dataset['text']\n",
    "    def clean_text(text):\n",
    "       \n",
    "        text = text.lower()\n",
    "        # Remove punctuation and numbers\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation + string.digits))\n",
    "        # split text into sentences\n",
    "        sentences = text.split('.')\n",
    "        \n",
    "        # split sentences to words\n",
    "        sentence_word_arrays = [sentence.split() for sentence in sentences if sentence]\n",
    "        \n",
    "        return sentence_word_arrays\n",
    "\n",
    "    # cleaning all text\n",
    "    dataset['tokenized_sentences'] = dataset['combined_text'].apply(clean_text)\n",
    "\n",
    "    return dataset['tokenized_sentences'].tolist()\n",
    "\n",
    "binary_sentences = prepare_dataset(dataset)\n",
    "for sentence_array in binary_sentences[:5]:\n",
    "    print(sentence_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d4fd16-3787-411e-b098-32a1de657f71",
   "metadata": {},
   "source": [
    "### Create A vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56e5ae3-712a-4a30-8b50-a7f5ac3cf684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(dataset) -> dict[str, int]:\n",
    " \n",
    "    vocabulary = {} \n",
    "    for sentences in dataset:\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word in vocabulary:\n",
    "                    vocabulary[word] += 1\n",
    "                else:\n",
    "                    vocabulary[word] = 1\n",
    "\n",
    "\n",
    "    sorted_vocabulary = dict(sorted(vocabulary.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    return sorted_vocabulary  \n",
    "vocabulary = create_vocabulary(binary_sentences)\n",
    "\n",
    "# Вывод первых 10 самых частых слов\n",
    "for word, frequency in list(vocabulary.items())[:40]:\n",
    "    print(f\"{word}: {frequency}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfa0b49-60b8-407e-9d6b-c5eef5f2e28e",
   "metadata": {},
   "source": [
    "### Word encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e66a17-f60e-4686-a360-2e4a483ed2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataset(dataset, vocabulary: dict[str, int]):\n",
    "    \"\"\" Encode each word in the dataset, based on your vocabulary \"\"\"\n",
    "    pass\n",
    "\n",
    "encoded_dataset = encode_dataset(dataset, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a41657-004a-4a20-9e11-f055656760ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fb906ef-e136-4b0a-a083-68370c27b530",
   "metadata": {},
   "source": [
    "Example using tensoflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a1208a-6ac2-4c39-a7c6-23338bc961f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# you can also use \"adapt\" \n",
    "vocabulary = create_vocabulary(dataset) \n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    max_tokens=...,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=...,\n",
    "    vocabulary=vocabulary\n",
    ")\n",
    "\n",
    "# adds padding and [UNK] token\n",
    "vocabulary = vectorize_layer.get_vocabulary()\n",
    "\n",
    "encoded_text_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n",
    "encoded_dataset = encoded_text_ds.as_numpy_iterator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af56a63-d20b-4292-8edf-95f27b029186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d1cb0a0-78a4-469a-8081-f325d97c8974",
   "metadata": {},
   "source": [
    "### Generate Positive and Negative Pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744b28fc-753a-420c-8e1b-3346e98c0c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(encoded_dataset, number_of_neg_samples: int):\n",
    "    \"\"\" \n",
    "    Generate positive and negative pairs \n",
    "    param: encoded_dataset:  the encoded dataset\n",
    "    param: number_of_neg_samples: Number of negative samples per positive pair \n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1d455e-f158-4de7-89ce-97d81f4060c0",
   "metadata": {},
   "source": [
    "Example using tensoflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c85fda-64c4-438c-b78b-14ee296b9602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# understand what is the functionalify of this, before you use it.\n",
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(len(vocabulary))\n",
    "\n",
    "\n",
    "# Generate positive skip-gram pairs for a sequence\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "      sequence,\n",
    "      vocabulary_size=VOCABULARY_SIZE,\n",
    "      sampling_table=sampling_table,\n",
    "      window_size=...,\n",
    "      negative_samples=0)\n",
    "\n",
    "\n",
    "# Iterate over each positive skip-gram pair to produce training examples\n",
    "# with a positive context word and negative samples.\n",
    "for target_word, context_word in positive_skip_grams:\n",
    "    \n",
    "    context_class = tf.expand_dims(tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "    negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "      true_classes=context_class,\n",
    "      num_true=1,\n",
    "      num_sampled=NUMBER_OF_NEGATIVE_SAMPLES,\n",
    "      unique=True,\n",
    "      range_max=VOCABULARY_SIZE,\n",
    "      seed=123,\n",
    "      name=\"negative_sampling\")\n",
    "\n",
    "    # Build context and label vectors (for one target word)\n",
    "    context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
    "    label = tf.constant([1] + [0] * NUMBER_OF_NEGATIVE_SAMPLES, dtype=\"int64\")\n",
    "\n",
    "    # Append each element from the training example to global lists.\n",
    "    targets.append(target_word)\n",
    "    contexts.append(context)\n",
    "    labels.append(label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c4ab2b-49f3-4a10-9236-c7da3fd79be6",
   "metadata": {},
   "source": [
    "### Define the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a245e822-508a-4491-9de5-31f1f0dbd238",
   "metadata": {},
   "outputs": [],
   "source": [
    " # define you model here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1de30c9-ba08-47aa-8cfb-bd05652d7aa8",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95b979f-9e85-40da-bfc5-86fb90f6e639",
   "metadata": {},
   "source": [
    "Example, there are other ways to train like using gradient tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91359a4b-2f3e-43b6-951c-34fa5c013fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=..., loss=..., metrics=[...])\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"model_logs\")\n",
    "\n",
    "# checkpoint a model. here we save the best model relative to validation loss\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(filepath=\"word2vec_model_w5_ns15_ckpt.h5\", monitor=..., save_best_only=True)\n",
    "\n",
    "# restore_best_weights - Whether to restore model weights from\n",
    "# the epoch with the best value of the monitored quantity.\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor='accuracy')\n",
    "\n",
    "history = model.fit(dataset, epochs=..., callbacks=[tensorboard_callback, checkpoint_cb, early_stopping_cb])\n",
    "\n",
    "model.save(f\"word2vec_model_w{WINDOW_SIZE}_ns{NEGATIVE_SAMPLES}.h5\", include_optimizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084dd0b9-d00b-431f-8c97-7284901421f6",
   "metadata": {},
   "source": [
    "### Save embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46778d77-bc61-4c4f-8d68-dc39650a2f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_embeddings(model, vocabulary: List[str], path: str):\n",
    "    \"\"\" \n",
    "    Save the embeddings \n",
    "    param: model: the trained tf model \n",
    "    param: vocabulary: list of tokens \n",
    "    param: path: the path to save the embeddings map \n",
    "\n",
    "    hint: take the weights using model.get_layer(...).get_weights()\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd18f62-3ec2-4707-98f5-8075e6ec8945",
   "metadata": {},
   "source": [
    "### Find Most similar\n",
    "\n",
    "Note that this must be efficient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466b69bd-718c-41e6-b9a2-3ff8a9b849bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar(word: str, k: int = 10) -> List[Tuple[str, float]]:\n",
    "    \"\"\" \n",
    "    Find most similar tokens to word \n",
    "    param: word: the word to find most similar words to \n",
    "    k: number of most similar words \n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa43790f-8dce-4aea-823d-811485bff72e",
   "metadata": {},
   "source": [
    "### Dimentionality Reduction \n",
    "\n",
    "visualize some clusters (pick some subset of words to show the labels for)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf194e2-78e5-41ee-af40-f7d846786c84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43faf1a7-67f4-4870-801e-7306dab0e57f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2316c74e-f996-4b4a-81dd-f9ae8f710fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
